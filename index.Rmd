---
title: "Statistical methods with applications to pairs trading and equipment life-time modeling"
#subtitle: "<s> How to make money in financial markets using stats </s>"
subtitle: "Ph.D. Defense"
author: "Allan Quadros"
date: "Ph.D. Candidate | Statistics </br> Kansas State University </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    # self_contained: true
    # pandoc_args: --wrap=preserve
    lib_dir: libs
    css: ["./css/kstate.css", "./css/kstate-fonts.css", "styles.css"]
    nature:
      beforeInit: ["./js/midd_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "./libs/partials/header.html"
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

layout: true
background-image: url(./img/logo/logo2.png)
background-position: 0% 100%
background-size: 5%

	
```{css echo=FALSE}
.highlight-last-item > ul > li,
.highlight-last-item > ol > li {
  opacity: 0.5;
}
.highlight-last-item > ul > li:last-of-type,
.highlight-last-item > ol > li:last-of-type {
  opacity: 1;
}
```


---
## Table of contents

<!-- <br> -->

<!-- > Objectives -->

<br>
<br>

> (1) Introduction <br><br>

> (2) Introduction to Pairs Trading Part <br>

> + (2.1) Bayesian Method <br>

> + (2.2) Non-Overlapping Block Bootstrap <br><br>

> (3) Introduction to Reliability Part <br>

> + (2.1) Predicting Equipment Life-cycle in the Absence of Data <br><br>

> (4) Conclusion <br><br>


---
class: inverse, center, middle
<!-- title-slide-section-grey,  -->

## Introduction to Pairs Trading

<!-- --- -->

<!-- </br> -->
<!-- </br> -->

<!-- ###<font color =#4C5455><code>What is (statistical) arbitrage?</code></font> -->

<!-- <!-- ??? Pairs trading is a type of statistical arbitrage. So we need to rewinf a little bit and understand what is statistical arbitrage. and to understand what is statistical arbitrage, we need to first understand what is arbitrage -->

<!-- <br> -->

<!-- > __Arbitrage__ <br><br> Take advantage of price differences in different markets for the same or different assets -->

<!-- <!-- ??? comecar pelo exemplo -->

<!-- <br> -->
<!-- <br> -->

<!-- > __Statistical arbitrage__ <br><br> When we use stats to do arbitrage -->


<!-- <!-- ??? we can use stats to identify assets, markets and the best time to trade with stats -->


---
class: highlight-last-item


### <code>__Pairs trading:__ <font color =#4C5455>what is it?</font></code>

</br>
</br>
</br>
--
+ Pairs trading is a investment strategy that exploits
temporary mispricings between two assets that historically __move together__.

</br>
--
+ When the pair deviates from its historical norm, investors __`BUY`__ (take a __long__ position in) the undervalued asset and __`SELL`__ (take a __short__ position in) the overvalued one, expecting a reversion of this spread to its historical average or levels.

</br>
--
+ Its main appeal lies in producing a __low-volatility__ and __market-neutral__ investment strategy.

<!-- pode ser bonds, contracts, options, stocks etc -->

<!-- </br> -->
<!-- -- -->
<!-- + It was first employed by a quantitative group at Morgan Stanley in the 1980s -->

<!-- </br> -->
<!-- -- -->
<!-- + It belongs to a broader class of investment strategy called statistical arbitrage - statistical modeling of price relationships among different assets to generate excess returns -->



---
###<font color =#4C5455><code>Example of co-moving assets: __Coca-Cola vs. Pepsi__</code></font>


```{r, message=FALSE, results='asis'}
library(plotly)
fig <- readRDS("plotly_LS2.rds")
fig
```


---
###<font color =#4C5455><code>__Pairs trading__: how to make money from it?</code></font>


```{r, fig.height=6, fig.width=10, fig.align='center', fig.retina=10}
# Carregar as bibliotecas necessárias
library(ggplot2)
library(dplyr)
library(tidyr)

# Definir os dados
x <- c(5.3, 5.4, 6, 7.2, 6.7, 5.5, 5.6)
y <- c(2.7, 2.8, 3.5, 2.5, 3.1, 3.6, 3.9)
dates <- as.Date(1:7, origin = "2023-01-01")  # Criar datas comuns

# Criar um data frame
df <- data.frame(dates, stock_X = x, stock_Y = y)

# Transformar para formato longo para ggplot2
df_long <- df %>%
  pivot_longer(cols = c(stock_X, stock_Y), names_to = "stock", values_to = "values")

# Criar o gráfico usando ggplot2
ggplot(df_long, aes(x = dates, y = values, color = stock, 
                    # linetype = stock
                    )) +
  geom_line(size = 1.2) +
  scale_color_manual(values = c("#602d89", "gray")) + # Define cores para cada série
  # scale_linetype_manual(values = c("solid", "dashed")) + # Define tipos de linha
  ylim(2, 8) +  # Limites do eixo y
  labs(title = "Pairs Trading Mechanism", x = "Date", y = "Values") +
  theme_minimal() +  # Tema minimalista
  theme(
    legend.position = "none",  # Remover a legenda
    plot.title = element_text(hjust = 0, vjust = 1, size = 12),  # Alinhar título à esquerda
    plot.margin = margin(t = 20, r = 20, b = 20, l = 20)  # Ajustar margens
  ) +
  # Adicionar o intervalo de datas no canto superior direito
  annotate("text", x = as.Date("2023-01-08"), y = 8.0, 
           label = "2023-01-02 / 2023-01-08", 
           hjust = 1, size = 3, color = "black") +
  # Adicionar anotações
  annotate("text", x = as.Date("2023-01-04"), y = 7.4, label = "Open\nshort position", size = 3.5, color = "black") +
  annotate("text", x = as.Date("2023-01-04"), y = 2.3, label = "Open\nlong position", size = 3.5, color = "black") +
  annotate("text", x = as.Date("2023-01-06"), y = 4.7, label = "Rewind\npositions", size = 3.5, color = "black")

```


---
class: highlight-last-item
###<font color =#4C5455><code>__Pairs trading__: what are the main challenges?</code></font>

<br>
<br>

> 1. __Identifying pairs__ of securities that exhibit a __stable relationship__ in the desired time frame. <br><br>

> 2. Optimal __share allocation__ between the two assets to appropriately hedge against market volatility. <br><br>
    
> 3. Generating __accurate trading signals__ to precisely time the entry and exit points. <br><br>


---
class: highlight-last-item
###<font color =#4C5455><code>__Pairs trading__: main adopted strategies</code></font>

<br>
<br>


> Distance Method;

> __Cointegration__;

> Copulas;

> Ornstein-Uhlenbeck;

> Machine Learning;

> Others.



---
###<font color =#4C5455><code>Cointegration strategy: __pair selection__</code></font>


<code> Cointegration test - a two step procedure designed by Engle & Granger (1987): </code> <br>


--
> __One:__ Select two stocks, say $X$ and $Y$, that historically __move together__ and test if the two price series ( $y_t$ and $x_t$ ) are __non-stationary__, i.e. both series have an unit root $\gamma = 0$; <br><br>


--
> __Two:__ Fit a linear model $\hat{y_t} = \hat{\beta_0} + \hat{\beta_1} x_t$, and test the __residuals__ ( $u_t$ ) for stationarity, i.e., test if $u_t$ does not have an unit root $\gamma < 0$.


<!-- ??? where $I(1)$ denotes an integrated process of order 1, meaning that the series becomes stationary only after taking the first difference. -->

<br>

--
+ If __`(1)`__ and __`(2)`__ hold, then the series are said to be __cointegrated__ - stocks $X$ and $Y$ share a __long-term equilibrium relationship__.



---
###<font color =#4C5455><code>Cointegration strategy: __trading__</code></font>

> Trading signals are generated based on the standard deviations of the spread $\hat{y_t} - \hat{\beta_1} x_t$

.pull-left[

<br>
<br>

> __Positions:__ whenever the Z-score $> \mid \pm k\sigma \mid$ thresholds

> __Sizing:__ go long (or short) $\hat{\beta_1}$ dollars of stock $X$ for each dollar of stock $Y$

> __Take-profit:__ when Z-score reverts back to `0` - the long-term average.

> __Stop-loss:__ when the Z-score reaches a defined loss margin ( $\pm|k\sigma + \xi|$ ).

]


.pull-right[

```{r, fig.height=6, fig.width=7, fig.align='center', fig.retina=4}
# fig.retina melhora a resolucao do plot
# ```{r, fig.height=6, fig.width=10, fig.align='center', fig.retina=4}
library(dplyr)
library(quantmod)
library(ggplot2)
library(gridExtra)

# Definir o período de coleta de dados
start_date <- as.Date("2023-06-01")
end_date <- Sys.Date()

# Obter os dados do Yahoo Finance
tickers <- c("KO", "PEP")

KO <- getSymbols(tickers[1], src = "yahoo", from = start_date, to = end_date, verbose = FALSE, auto.assign = FALSE)

PEP <- getSymbols(tickers[2], src = "yahoo", from = start_date, to = end_date, verbose = FALSE, auto.assign = FALSE)

# Extrair os preços de fechamento e calcular o log dos preços
KO_log_prices <- log(Cl(KO))
PEP_log_prices <- log(Cl(PEP))

# Criar um dataframe com os preços em log
prices_df <- data.frame(
  Date = index(KO_log_prices),
  KO = as.numeric(KO_log_prices),
  PEP = as.numeric(PEP_log_prices - 0.6)
)

mod <- lm(KO ~ PEP, data = prices_df)

# Calcular o spread entre os preços em log e o z-score
prices_df <- prices_df %>%
  mutate(
    Spread = KO - KO - (coef(mod)[2] * PEP),
    Zscore = (Spread - mean(Spread)) / sd(Spread)
  )

# Gráfico 1: Preços históricos em log de KO e PEP com legenda abaixo
p1 <- ggplot(prices_df, aes(x = Date)) +
  geom_line(aes(y = KO, color = "KO"), size = 0.6) +
  geom_line(aes(y = PEP, color = "PEP"), size = 0.6) +
  scale_color_manual(values = c("KO" = "#602d89", "PEP" = "gray")) +
  labs(title = "Pair: KO vs. PEP", y = "Log Price", x = NULL) +
  theme_minimal() +
  theme(
    legend.title = element_blank(),
    legend.position = "bottom"  # Posiciona a legenda na parte inferior
  )

# Gráfico 2: Z-score do spread com linhas em -2 e +2 desvios padrão
# Gráfico 2: Z-score do spread com linhas em -2 e +2 desvios padrão e novas cores
p2 <- ggplot(prices_df, aes(x = Date, y = Zscore)) +
  geom_line(color = "black", size = 0.6) +  # Azul suave
  geom_hline(yintercept = c(-2, 2), linetype = "dashed", color = "darkgray") +  # Verde escuro
  geom_hline(yintercept = c(-3, 0, 3), linetype = "dashed", color = "#4a90e2") +  # Azul suave
  labs(title = "Z-score of Spread", y = "Z-score", x = "Date") +
  theme_minimal()

# Combinar os gráficos
grid.arrange(p1, p2, ncol = 1, heights = c(2, 1))


```


]

<!-- ??? LEGAL: O alpha nao precisa entrar no calculo justamente pelo motivo do qual eu distorci o primeiro grafico com log(PEP) - c. Sem o c, o efeito seria o mesmo, ou sejam nao interessa a distancia absoluta entre KO e PEP - o que interesse eh o desvio relativo entre ambas e nao o desvio total considerando o nivel inicial. Alem do fato de que ao icnluirmos alpha, teremos mais uma variavel para estimar tornando o modelo mais complexo e prone to more errors and overfitting-->


<!-- ??? falar do zscore e mostrar o grafico 2 (mudar as cores - usar o codigo do palomar) - falar sobre transformar as duas series em um ativo sintetico -->


---
###<font color =#4C5455><code> Proposed methods: __motivation__ </code></font>


<code> Problems with the standard cointegration method: </code></br></br>


--
> High false positive rates when used within a data mining approach;</br></br>

--
> Has been widely adopted by both institutional and individual investors;</br></br>

--
> Can result in premature/delayed exits.

<!-- ??? alpha level is inflated -->

<!-- ??? less opportunities - the margins have shrunk -->

<!-- ??? practically speaking, relying only on the zscore of the spread is not very effective -- sometimes the relationship between the securities changes and the zscore does not capture that. Soetimes, it is too sensitive. -->

<!-- ??? Competes against high frequency trading to identify distortions ? -->

---
###<font color =#4C5455><code> Proposed methods: __hypotheses__ </code></font>

> __Idea:__ $\hat{\beta_1}$ carries important information about the linear relationship between the underlying __co-moving__ securities.</br></br>

--
> __Proposal:__ Add a __confirmation layer__ to the standard cointegration strategy in pairs trading by evaluating the behavior of the __hedge ratio__ $\hat{\beta_1}$.</br></br>

--
> __How:__ Deriving the distribution of $\hat{\beta}_1$ from OLS regression through a __parametric__ or __non-parametric method__, and establishing two thresholds within this distribution that will serve as control/confirmation boundaries for the trading strategy.</br></br>

--
<code> __Hypotheses:__  </code>
  
  > __(i)__ produces more accurate trading signals;
  
  > __(ii)__ increases false discovery rates (FDR) in cointegration test. __ $^{(***)}$ __


---
class: inverse, center, middle
<!-- title-slide-section-grey,  -->
## Bayesian Method

---
###<font color =#4C5455><code> Bayesian method: __mechanics__ </code></font>


```{r, fig.height=7, fig.width=12, fig.align='center', fig.retina=12, results='hold'}

# Set seed for reproducibility
set.seed(123)

# Function to create the distribution plot with different alpha values using base R
create_beta1_distribution_plot_base_r <- function(alpha_small = 0.05, alpha_large = 0.15) {
  # Configurar o dispositivo gráfico com 3 regiões: gráfico superior, legenda, gráfico inferior
  # Salvar parâmetros gráficos originais
  old_par <- par(no.readonly = TRUE)
  
  # Definir as margens externas (outer margins) para acomodar o título geral
  par(oma = c(0, 0, 3, 0))  # Adiciona 3 linhas de margem no topo para o título geral
  
  # Configurar layout com 3 painéis: gráfico superior, legenda, gráfico inferior
  layout(matrix(c(1, 2, 3), nrow = 3, ncol = 1), heights = c(4, 0.5, 4))
  
  # Configurar margens para o gráfico superior
  par(mar = c(4, 4, 5, 5) + 0.9)
  
  # Create a sequence for x values of the truncated normal distribution (β₁ > 0)
  x <- seq(0, 4, length.out = 1000)
  
  # Calculate truncated normal distribution values
  # Density function for truncated normal (μ = 0, σ = 1, lower bound = 0)
  truncated_normal <- function(x, mean = 0, sd = 1) {
    # For x > 0, the density is 2*dnorm(x) to account for truncation
    2 * dnorm(x, mean, sd)
  }
  
  y <- truncated_normal(x)
  
  # Calculate quantiles for different alpha values
  # For truncated normal, we need to adjust the quantiles to reflect β₁ > 0
  # These are custom values to demonstrate the concept visually
  q_small_low <- qnorm(alpha_small*2, 0, 1) # Adjust for truncation
  q_small_high <- qnorm(1 - alpha_small/2, 0, 1) # Adjust for truncation
  q_large_low <- qnorm(alpha_large*2, 0, 1) # Adjust for truncation
  q_large_high <- qnorm(1 - alpha_large/2, 0, 1) # Adjust for truncation
  
  # Make sure lower quantiles are positive for the truncated normal
  q_small_low <- max(0.1, q_small_low)
  q_large_low <- max(0.3, q_large_low)
  
  # First plot (smaller alpha)
  # plot(x, y, type = "n", xlab = expression(paste("Values of ", beta[1])), ylab = "Density",
  #      main = bquote(paste(alpha, " = ", .(alpha_small))),
  #      cex.main = 2.6, cex.lab = 2, font.main = 2, bty = "n", xaxt = "n")
  
  plot(x, y, type = "n", xlab = "", ylab = "Density",
       main = bquote(paste(alpha, " = ", .(alpha_small))),
       cex.main = 2.6, cex.lab = 2, font.main = 2, bty = "n", xaxt = "n")
  
  # Add subtitle
  mtext("Wider well-behaved region \n(less selective entry, more selective exit)", 
        side = 3, line = -7.5, at = 3, cex = 1.4, font = 3)
  
  # Fill the well-behaved region (middle area)
  # For truncated normal, lower bound is 0
  lower_bound <- max(0, q_small_low)
  x_middle <- x[x >= lower_bound & x <= q_small_high]
  y_middle <- y[x >= lower_bound & x <= q_small_high]
  
  # Draw hatched orange area for well-behaved region
  # First, fill with solid very light orange
  polygon(c(x_middle[1], x_middle, x_middle[length(x_middle)]), 
          c(0, y_middle, 0), col = rgb(0.9, 0.9, 1), border = NA)
  
  par(xpd = TRUE)
  arrows(x0 = 2.2, y0 = 0.57, x1 = 1.5, y1 = 0.45, 
         length = 0.15, angle = 20, code = 2, 
         lwd = 2, col = "black")
  par(xpd = FALSE)
  
  # Then add hatching with orange lines
  if (length(x_middle) > 10) {
    # Calculate interval between lines based on region width
    region_width <- q_small_high - lower_bound
    line_interval <- region_width / 15  # 15 lines in the region
    
    # Draw diagonal orange lines for hatching
    for (i in seq(lower_bound, q_small_high, by = line_interval)) {
      # Get max height at this x position
      height_idx <- which.min(abs(x - i))
      max_height <- y[height_idx]
      
      # Draw the line
      lines(c(i, i), c(0, max_height), col = "blue", lty = 1, lwd = 1)
    }
  }
  
  # For truncated normal, only show buy signal if q_small_low > 0
  if (q_small_low > 0) {
    x_left <- x[x < q_small_low & x >= 0]
    y_left <- y[x < q_small_low & x >= 0]
    if (length(x_left) > 0) {
      # Fill with light gray instead of light red
      polygon(c(x_left[1], x_left, x_left[length(x_left)]), 
              c(0, y_left, 0), col = rgb(0.9, 0.9, 0.9), border = NA)
    }
  }
  
  # Fill the sell signal region (right area) with light gray
  x_right <- x[x > q_small_high]
  y_right <- y[x > q_small_high]
  polygon(c(x_right[1], x_right, x_right[length(x_right)]), 
          c(0, y_right, 0), col = rgb(0.9, 0.9, 0.9), border = NA)
  
  # Add the distribution curve
  lines(x, y, lwd = 2)
  
  # Add vertical lines for quantiles - update color
  abline(v = q_small_low, lty = 2, col = "gray40", lwd = 2)
  abline(v = q_small_high, lty = 2, col = "gray40", lwd = 2)
  
  # Add colored regions along the x-axis to highlight different zones - make thicker
  add_color_highlight <- function(x_start, x_end, color) {
    # Create a thicker rectangular region
    rect(x_start, -0.04, x_end, 0, col = color, border = NA)
  }
  
  # Add the colored highlights for the different regions
  if (q_small_low > 0) {
    # Buy signal region (left)
    add_color_highlight(0, q_small_low, "gray40")
  }
  
  # Well-behaved region (middle)
  add_color_highlight(max(0, q_small_low), q_small_high, "blue")
  
  # Sell signal region (right)
  add_color_highlight(q_small_high, 4, "gray40")
  
  # Add quantile labels - placed below x-axis
  if (q_small_low > 0) {
    # Draw vertical line at lower quantile
    lines(c(q_small_low, q_small_low), c(0, 0.05), col = "gray40", lty = 2, lwd = 1.5)
    # Add label below the axis - use xpd=TRUE to allow drawing outside the plot region
    par(xpd = TRUE)
    text(q_small_low, -0.03, bquote(paste("q"[alpha], " = q"[.(alpha_small)])), 
         cex = 2.2, col = "black", pos = 1)
    par(xpd = FALSE)
  }
  
  # Draw vertical line at upper quantile
  lines(c(q_small_high, q_small_high), c(0, 0.05), col = "gray40", lty = 2, lwd = 1.5)
  # Add label below the axis
  par(xpd = TRUE)
  text(q_small_high, -0.03, bquote(paste("q"[1-alpha], " = q"[.(1-alpha_small)])), 
       cex = 2.2, col = "black", pos = 1)
  par(xpd = FALSE)
  
  # Painel central para a legenda
  # Configurar margens mínimas para o painel da legenda
  par(mar = c(0, 0, 0, 0))
  
  # Criar um plot vazio para a legenda
  plot(0, 0, type = "n", axes = FALSE, xlab = "", ylab = "", xlim = c(0, 1), ylim = c(0, 1))
  
  # Adicionar a legenda centralizada
  legend("center", 
         legend = c(
           expression(paste("well-behaved ", beta[1], " interval (confirmatory region for entry signals)")),
           expression(paste("ill-behaved ", beta[1], " interval (confirmatory region for stop-loss signals)"))
         ),
         fill = c("blue", "gray40"),
         border = NA,
         bty = "n",
         cex = 1.82,
         pt.cex = 10,
         horiz = FALSE)
  
  # Configurar margens para o gráfico inferior - adicionando espaço para o eixo x
  par(mar = c(3, 4, 5, 5) + 1)
  
  # Second plot (larger alpha)
  plot(x, y, type = "n", xlab = expression(paste("Values of ", beta[1])), ylab = "Density",
       main = bquote(paste(alpha, " = ", .(alpha_large))), 
       cex.main = 2.6, cex.lab = 2, font.main = 2, bty = "n", xaxt = "n")
  
  # Add subtitle
  mtext("Narrower well-behaved region \n(more selective entry, less selective exit)", 
        side = 3, line = -7.5, at = 2.5, cex = 1.4, font = 3)
  
  # Fill the well-behaved region (middle area) with hatched orange
  lower_bound <- max(0, q_large_low)
  x_middle <- x[x >= lower_bound & x <= q_large_high]
  y_middle <- y[x >= lower_bound & x <= q_large_high]
  
  # First, fill with solid very light orange
  polygon(c(x_middle[1], x_middle, x_middle[length(x_middle)]), 
          c(0, y_middle, 0), col = rgb(0.9, 0.9, 1), border = NA)
  
  par(xpd = TRUE)
  arrows(x0 = 1.63, y0 = 0.57, x1 = 1.25, y1 = 0.52, 
         length = 0.15, angle = 20, code = 2, 
         lwd = 2, col = "black")
  par(xpd = FALSE)
  
  # Then add hatching with orange lines
  if (length(x_middle) > 10) {
    # Calculate interval between lines based on region width
    region_width <- q_large_high - lower_bound
    line_interval <- region_width / 15  # 15 lines in the region
    
    # Draw vertical orange lines for hatching
    for (i in seq(lower_bound, q_large_high, by = line_interval)) {
      # Get max height at this x position
      height_idx <- which.min(abs(x - i))
      max_height <- y[height_idx]
      
      # Draw the line
      lines(c(i, i), c(0, max_height), col = "blue", lty = 1, lwd = 1)
    }
  }
  
  # For truncated normal, only show buy signal if q_large_low > 0
  if (q_large_low > 0) {
    x_left <- x[x < q_large_low & x >= 0]
    y_left <- y[x < q_large_low & x >= 0]
    if (length(x_left) > 0) {
      # Fill with light gray instead of light red
      polygon(c(x_left[1], x_left, x_left[length(x_left)]), 
              c(0, y_left, 0), col = rgb(0.9, 0.9, 0.9), border = NA)
    }
  }
  
  # Fill the sell signal region (right area) with light gray
  x_right <- x[x > q_large_high]
  y_right <- y[x > q_large_high]
  polygon(c(x_right[1], x_right, x_right[length(x_right)]), 
          c(0, y_right, 0), col = rgb(0.9, 0.9, 0.9), border = NA)
  
  # Add the distribution curve
  lines(x, y, lwd = 2)
  
  # Add vertical lines for quantiles - update color
  abline(v = q_large_low, lty = 2, col = "gray40", lwd = 2)
  abline(v = q_large_high, lty = 2, col = "gray40", lwd = 2)
  
  # Add colored regions along the x-axis for the second plot too
  if (q_large_low > 0) {
    # Buy signal region (left)
    add_color_highlight(0, q_large_low, "gray40")
  }
  
  # Well-behaved region (middle)
  add_color_highlight(max(0, q_large_low), q_large_high, "blue")
  
  # Sell signal region (right)
  add_color_highlight(q_large_high, 4, "gray40")
  
  # Add quantile labels below x-axis for second plot
  if (q_large_low > 0) {
    # Draw vertical line at lower quantile
    lines(c(q_large_low, q_large_low), c(0, 0.05), col = "gray40", lty = 2, lwd = 1.5)
    # Add label below the axis
    par(xpd = TRUE)
    text(q_large_low, -0.03, bquote(paste("q"[alpha], " = q"[.(alpha_large)])), 
         cex = 2.2, col = "black", pos = 1)
    par(xpd = FALSE)
  }
  
  # Draw vertical line at upper quantile
  lines(c(q_large_high, q_large_high), c(0, 0.05), col = "gray40", lty = 2, lwd = 1.5)
  # Add label below the axis
  par(xpd = TRUE)
  text(q_large_high, -0.03, bquote(paste("q"[1-alpha], " = q"[.(1-alpha_large)])), 
       cex = 2.2, col = "black", pos = 1)
  par(xpd = FALSE)
  
  # Add two-line title with atop()
  # mtext(expression(atop("Quantile Parameter " * alpha * " and 'Well-behaved' Intervals",
  #                       "under the Full Conditional of " * beta[1])),
  #       side = 3, line = -2.3, outer = TRUE, cex = 1.7, font = 2)
  mtext(expression(atop("Full Conditional Distribution of " * beta[1])),
        side = 3, line = -2.3, outer = TRUE, cex = 1.7, font = 2)
  
  # Reset to default
  par(old_par)
}

# Exemplo de uso
create_beta1_distribution_plot_base_r(alpha_small = 0.05, alpha_large = 0.15)

```






---
###<font color =#4C5455><code> Theoretical Background: __hierarchical Bayesian model__ </code></font>

<code> Using results from Gelfand _et al._ (1992), Hooten & Hefley (2019), and Rencher (2007), we have: </code>

--
<code> __Likelihood:__ </code>

.my-style2[
> $$y_i \sim \mathcal{N}(\boldsymbol{X\beta}, \sigma^2)$$

]

.my-style[
where $\boldsymbol{\beta} = \begin{bmatrix} \beta_0 & \beta_1 \end{bmatrix}^\top$
]


--
<code> __Priors:__ </code>
.my-style2[
> $$\boldsymbol{\beta} \sim \mathcal{TN}(\boldsymbol{\mu_{\beta}} = \mathbf{(X'X)^{-1}X'y}, \boldsymbol{\Sigma_{\beta}} = \sigma_0^2 \mathbf{(X'X)^{-1}}, \phantom{.}0, +\infty)$$ 

]

.my-style[
where $\widehat{\sigma_0^2} = \frac{1}{n-2}\boldsymbol{y'(I - H)y}$ and $\boldsymbol{H = X(X'X)^{-1}X'}$

]

.my-style2[
>  $$\sigma^2 \sim \mathcal{IG}(q, r)$$

]


--
<code> __Conjugate full-conditional posteriors:__ </code>
.my-style2[
> $$\boldsymbol{\beta} \mid \boldsymbol{y}, \sigma^2 \equiv \mathcal{TN}(\boldsymbol{\mu} = \boldsymbol{A^{-1}b}, \boldsymbol{\Sigma} = \boldsymbol{A^{-1}}, \phantom{.}0, +\infty)$$

]

.my-style[
where $\boldsymbol{A} \equiv \boldsymbol{X}^\top (\sigma^2\boldsymbol{I})^{-1} \boldsymbol{X} + \boldsymbol{\Sigma_\beta}^{-1}$ and $\boldsymbol{b} \equiv \boldsymbol{X}^\top (\sigma^2\boldsymbol{I})^{-1} \boldsymbol{y} + \boldsymbol{\Sigma_\beta}^{-1} \boldsymbol{\mu_\beta}$
]

.my-style2[
> $$\sigma^2 \mid \boldsymbol{y, \beta} \equiv \mathcal{IG}(\tilde q, \tilde r)$$

]

.my-style[
where $\tilde q = q + \frac{n}{2}$ and $\tilde r = \left[\frac{1}{2}\boldsymbol{(y - X\beta)'(y - X\beta)} + \frac{1}{r}\right]^{-1}$
]

---
class: inverse, center, middle
<!-- title-slide-section-grey,  -->

## Results for the Bayesian Method


---
###<font color =#4C5455><code> Empirical Results: __data & methodology__ </code></font>

<br>
<br>

--
> Alpha Vantage and MT5 data, 30-minute timeframe; <br><br>

--
> 2 years of data: __`02-01-2023 - 01-31-2025`__;<br><br>

--
> Backtest consisted of sliding window with 3 periods: forming `[t-1]` | decision `[t]` | trading `[t+1]`;<br><br>

--
> Parameters: sizes $w \in \{65, 130\}$ and quantile parameter $\alpha \in \{0.05, 0.15\}$; <br><br>

--
> The algorithm was implemented in R


---
###<font color =#4C5455><code> Empirical Results: __backtesting__ </code></font>

<iframe src="animated_z_score_with_spike_and_stabilization6.html" width="750" height="520" style="border:none;"></iframe>


---
###<font color =#4C5455><code> Empirical Results: __U.S.__ </code></font>

<br>
<br>
<br>

.center[

[__LINK__](./results/US2.html)

]



---
###<font color =#4C5455><code> Empirical Results: __Brazil__ </code></font>


<br>
<br>
<br>

.center[

[__LINK__](./results/Brazil2.html)

]


---
###<font color =#4C5455><code> Simulation Study: __pair selection__ </code></font>

Performance of the Bayesian method in correctly rejecting false positives.

```{r, echo=FALSE}
library(knitr)
library(kableExtra)

# Criando o dataframe com os dados
tabela <- data.frame(
  Beta = c(rep(" ", 3), rep(" ", 3), rep(" ", 3), rep(" ", 3), rep(" ", 3), rep(" ", 3)),
  Metric = rep(c("False Positives", "Correctly Rejected", "Proportion"), 6),
  `w500` = c(73,43,0.59, 73,44,0.60, 76,43,0.57, 71,41,0.58, 75,45,0.60, 368,216,0.587),
  `w252` = c(36,17,0.47, 36,19,0.53, 33,16,0.48, 37,20,0.54, 35,17,0.49, 177,89,0.503),
  `w180` = c(70,70,1.00, 70,70,1.00, 70,70,1.00, 70,70,1.00, 72,72,1.00, 352,352,1.00),
  `w120` = c(71,70,0.99, 72,72,1.00, 72,72,1.00, 71,70,0.99, 71,70,0.99, 357,354,0.99),
  `w90`  = c(52,35,0.67, 34,17,0.50, 68,17,0.25, 37,18,0.49, 17,0,0.00, 208,87,0.418),
  `w60`  = c(0,0,"-", 2,1,0.50, 0,0,"-", 1,0,0.00, 0,0,"-", 3,1,0.33),
  Total = c(302,235,0.78, 287,223,0.78, 319,218,0.68, 287,219,0.76, 270,204,0.76, 1465,1099,0.75)
)

# Gerando a tabela bonita em HTML com paginação
tabela %>%
  kable("html", digits = 1#, 
        #caption = "Performance of the Bayesian method in correctly rejecting false positives."
        ) %>%
  kable_styling("striped", full_width = F, position = "center") %>%
  pack_rows("β₁ = 0.50", 1, 3) %>%
  pack_rows("β₁ = 0.75", 4, 6) %>%
  pack_rows("β₁ = 1.00", 7, 9) %>%
  pack_rows("β₁ = 1.25", 10, 12) %>%
  pack_rows("β₁ = 1.50", 13, 15) %>%
  pack_rows("Overall Totals", 16, 18) %>%
  scroll_box(height = "400px")
```



---
class: inverse, center, middle
<!-- title-slide-section-grey,  -->

## Non-Overlapping Block Bootstrap



---
###<font color =#4C5455><code> NBB method: __mechanics__ </code></font>


```{r, fig.height=7, fig.width=12, fig.align='center', fig.retina=12, results='hold'}
# Set seed for reproducibility
set.seed(123)

# Function to create the normal distribution plot with different alpha values
create_beta1_distribution_plot_normal <- function(mean_beta = 1.5, sd_beta = 0.3, alpha_small = 0.05, alpha_large = 0.15) {
  # Save original graphical parameters
  old_par <- par(no.readonly = TRUE)
  
  # Define outer margins to accommodate the general title
  par(oma = c(0, 0, 3, 0))  # Add 3 lines of margin at the top for the general title
  
  # Configure layout with 3 panels: upper graph, legend, lower graph
  layout(matrix(c(1, 2, 3), nrow = 3, ncol = 1), heights = c(4, 0.5, 4))
  
  # Configure margins for the upper graph
  par(mar = c(4, 4, 5, 5) + 0.9)
  
  # Create a sequence for x values of the normal distribution
  x <- seq(mean_beta - 4*sd_beta, mean_beta + 4*sd_beta, length.out = 1000)
  
  # Calculate normal distribution values
  y <- dnorm(x, mean = mean_beta, sd = sd_beta)
  
  # Calculate quantiles for different alpha values
  q_small_low <- qnorm(alpha_small, mean = mean_beta, sd = sd_beta)
  q_small_high <- qnorm(1 - alpha_small, mean = mean_beta, sd = sd_beta)
  q_large_low <- qnorm(alpha_large, mean = mean_beta, sd = sd_beta)
  q_large_high <- qnorm(1 - alpha_large, mean = mean_beta, sd = sd_beta)
  
  # First plot (smaller alpha)
  plot(x, y, type = "n", xlab = "", ylab = "Density",
       main = bquote(paste(alpha, " = ", .(alpha_small))),
       cex.main = 2.6, cex.lab = 2, font.main = 2, bty = "n", xaxt = "n")
  
  # Add subtitle
  mtext("Wider well-behaved region \n(less selective entry,\nmore selective exit)", 
        side = 3, line = -7.5, at = mean_beta + 3.5*sd_beta, cex = 1.4, font = 3)
  
  # Fill the well-behaved region (middle area)
  x_middle <- x[x >= q_small_low & x <= q_small_high]
  y_middle <- y[x >= q_small_low & x <= q_small_high]
  
  # Fill with solid very light orange
  polygon(c(x_middle[1], x_middle, x_middle[length(x_middle)]), 
          c(0, y_middle, 0), col = rgb(1, 0.9, 0.8), border = NA)
  
  # Add explanatory arrows
  par(xpd = TRUE)
  arrows(x0 = mean_beta + 2*sd_beta, y0 = max(y) * 0.7, x1 = mean_beta + 1.3*sd_beta, y1 = max(y) * 0.5, 
         length = 0.15, angle = 20, code = 2, 
         lwd = 2, col = "black")
  par(xpd = FALSE)
  
  # Add hatching with orange lines
  if (length(x_middle) > 10) {
    # Calculate interval between lines based on region width
    region_width <- q_small_high - q_small_low
    line_interval <- region_width / 15  # 15 lines in the region
    
    # Draw vertical orange lines for hatching
    for (i in seq(q_small_low, q_small_high, by = line_interval)) {
      # Get max height at this x position
      height_idx <- which.min(abs(x - i))
      max_height <- y[height_idx]
      
      # Draw the line
      lines(c(i, i), c(0, max_height), col = "darkorange", lty = 1, lwd = 1)
    }
  }
  
  # Fill the buy signal region (left) with light gray
  x_left <- x[x < q_small_low]
  y_left <- y[x < q_small_low]
  polygon(c(x_left[1], x_left, x_left[length(x_left)]), 
          c(0, y_left, 0), col = rgb(0.9, 0.9, 0.9), border = NA)
  
  # Fill the sell signal region (right) with light gray
  x_right <- x[x > q_small_high]
  y_right <- y[x > q_small_high]
  polygon(c(x_right[1], x_right, x_right[length(x_right)]), 
          c(0, y_right, 0), col = rgb(0.9, 0.9, 0.9), border = NA)
  
  # Add the distribution curve
  lines(x, y, lwd = 2)
  
  # Add vertical lines for quantiles
  abline(v = q_small_low, lty = 2, col = "gray40", lwd = 2)
  abline(v = q_small_high, lty = 2, col = "gray40", lwd = 2)
  
  # Function to add color highlight along x-axis
  add_color_highlight <- function(x_start, x_end, color) {
    # Create a thicker rectangular region
    rect(x_start, -0.04 * max(y), x_end, 0, col = color, border = NA)
  }
  
  # Add the colored highlights for the different regions
  add_color_highlight(min(x), q_small_low, "gray40")
  add_color_highlight(q_small_low, q_small_high, "darkorange")
  add_color_highlight(q_small_high, max(x), "gray40")
  
  # Add quantile labels - placed below x-axis
  # Draw vertical line at lower quantile
  lines(c(q_small_low, q_small_low), c(0, 0.05 * max(y)), col = "gray40", lty = 2, lwd = 1.5)
  # Add label below the axis
  par(xpd = TRUE)
  text(q_small_low, -0.03 * max(y), bquote(paste("q"[alpha], " = q"[.(alpha_small)])), 
       cex = 2.2, col = "black", pos = 1)
  par(xpd = FALSE)
  
  # Draw vertical line at upper quantile
  lines(c(q_small_high, q_small_high), c(0, 0.05 * max(y)), col = "gray40", lty = 2, lwd = 1.5)
  # Add label below the axis
  par(xpd = TRUE)
  text(q_small_high, -0.03 * max(y), bquote(paste("q"[1-alpha], " = q"[.(1-alpha_small)])), 
       cex = 2.2, col = "black", pos = 1)
  par(xpd = FALSE)
  
  # Central panel for the legend
  # Configure minimal margins for the legend panel
  par(mar = c(0, 0, 0, 0))
  
  # Create an empty plot for the legend
  plot(0, 0, type = "n", axes = FALSE, xlab = "", ylab = "", xlim = c(0, 1), ylim = c(0, 1))
  
  # Add the centered legend
  legend("center", 
         legend = c(
           expression(paste("well-behaved ", beta[1], " interval (confirmatory region for entry signals)")),
           expression(paste("ill-behaved ", beta[1], " interval (confirmatory region for stop-loss signals)"))
         ),
         fill = c("darkorange", "gray40"),
         border = NA,
         bty = "n",
         cex = 1.63,
         pt.cex = 10,
         horiz = FALSE)
  
  # Configure margins for the lower graph
  par(mar = c(3, 4, 5, 5) + 1)
  
  # Second plot (larger alpha)
  plot(x, y, type = "n", xlab = expression(paste("Values of ", beta[1])), ylab = "Density",
       main = bquote(paste(alpha, " = ", .(alpha_large))), 
       cex.main = 2.6, cex.lab = 2, font.main = 2, bty = "n", xaxt = "n")
  
  # Add subtitle
  mtext("Narrower well-behaved region \n(more selective entry,\nless selective exit)", 
        side = 3, line = -7.5, at = mean_beta + 3.3*sd_beta, cex = 1.4, font = 3)
  
  # Fill the well-behaved region (middle area) with hatched orange
  x_middle <- x[x >= q_large_low & x <= q_large_high]
  y_middle <- y[x >= q_large_low & x <= q_large_high]
  
  # First, fill with solid very light orange
  polygon(c(x_middle[1], x_middle, x_middle[length(x_middle)]), 
          c(0, y_middle, 0), col = rgb(1, 0.9, 0.8), border = NA)
  
  
  
  # Then add hatching with orange lines
  if (length(x_middle) > 10) {
    # Calculate interval between lines based on region width
    region_width <- q_large_high - q_large_low
    line_interval <- region_width / 15  # 15 lines in the region
    
    # Draw vertical orange lines for hatching
    for (i in seq(q_large_low, q_large_high, by = line_interval)) {
      # Get max height at this x position
      height_idx <- which.min(abs(x - i))
      max_height <- y[height_idx]
      
      # Draw the line
      lines(c(i, i), c(0, max_height), col = "darkorange", lty = 1, lwd = 1)
    }
  }
  
  # Fill the buy signal region (left) with light gray
  x_left <- x[x < q_large_low]
  y_left <- y[x < q_large_low]
  polygon(c(x_left[1], x_left, x_left[length(x_left)]), 
          c(0, y_left, 0), col = rgb(0.9, 0.9, 0.9), border = NA)
  
  # Fill the sell signal region (right) with light gray
  x_right <- x[x > q_large_high]
  y_right <- y[x > q_large_high]
  polygon(c(x_right[1], x_right, x_right[length(x_right)]), 
          c(0, y_right, 0), col = rgb(0.9, 0.9, 0.9), border = NA)
  
  # Add the distribution curve
  lines(x, y, lwd = 2)
  
  # Add vertical lines for quantiles
  abline(v = q_large_low, lty = 2, col = "gray40", lwd = 2)
  abline(v = q_large_high, lty = 2, col = "gray40", lwd = 2)
  
  # Add colored regions along the x-axis for the second plot too
  add_color_highlight(min(x), q_large_low, "gray40")
  add_color_highlight(q_large_low, q_large_high, "darkorange")
  add_color_highlight(q_large_high, max(x), "gray40")
  
  # Add quantile labels below x-axis for second plot
  # Draw vertical line at lower quantile
  lines(c(q_large_low, q_large_low), c(0, 0.05 * max(y)), col = "gray40", lty = 2, lwd = 1.5)
  # Add label below the axis
  par(xpd = TRUE)
  text(q_large_low, -0.03 * max(y), bquote(paste("q"[alpha], " = q"[.(alpha_large)])), 
       cex = 2.2, col = "black", pos = 1)
  par(xpd = FALSE)
  
  # Draw vertical line at upper quantile
  lines(c(q_large_high, q_large_high), c(0, 0.05 * max(y)), col = "gray40", lty = 2, lwd = 1.5)
  # Add label below the axis
  par(xpd = TRUE)
  text(q_large_high, -0.03 * max(y), bquote(paste("q"[1-alpha], " = q"[.(1-alpha_large)])), 
       cex = 2.2, col = "black", pos = 1)
  par(xpd = FALSE)
  
  # Add two-line title
  mtext(expression(atop("Empirical Distribution of " * beta[1])),
        side = 3, line = -2.3, outer = TRUE, cex = 1.7, font = 2)
  
  # Add explanatory arrows
  par(xpd = TRUE)
  arrows(x0 = mean_beta + 1.7*sd_beta, y0 = max(y) * 0.6, x1 = mean_beta + 1*sd_beta, y1 = max(y) * 0.4, 
         length = 0.15, angle = 20, code = 2, 
         lwd = 2, col = "black")
  par(xpd = FALSE)
  # Reset to default
  par(old_par)
}

# Run the function with the specified parameters
create_beta1_distribution_plot_normal(mean_beta = 1.5, sd_beta = 0.3, alpha_small = 0.05, alpha_large = 0.15)
```




---
###<font color =#4C5455><code> Theoretical Background: __non-overlapping block bootstrap__ </code></font>

<code> Inspired in Lahiri (2003), we have: </code>

<br>

--
<code> __Bootstrap sample__ </code> $\leftrightarrow$ <code> __Original sample__: </code>

$$(W^*_{(j-1)b+1}, W^*_{(j-1)b+2}, \ldots, W^*_{jb}) = B_{I_j} = (W_{(I_j-1)b+1}, W_{(I_j-1)b+2}, \ldots, W_{I_jb})$$

.my-style2[
for $j = 1, 2, \ldots, k$
]


Where:

<!-- > j is a specific block; -->

> $b$ is the block size;

> $W$ represents an observation in each type of sample;

> left side represents j-th block from bootstrap sample;

> right side represents $I_j$-th block from original sample

> $I_1, I_2, \ldots, I_k$ are random indices indicating which original block will be used to form the j-th bootstrap block;

<!-- ??? iterate over the possible values  -->


---
###<font color =#4C5455><code> Theoretical Background: __non-overlapping block bootstrap__ </code></font>

<br>
<br>
<br>

<code> __Adaptation for partial end blocks:__ </code>

\begin{equation}
B_j=\begin{cases}(W_{(j-1)b+1}, W_{(j-1)b+2}, \ldots, W_{jb}), & \text{if } j < k' \\ (W_{(j-1)b+1}, W_{(j-1)b+2}, \ldots, W_n), & \text{if } j = k'\end{cases}
\end{equation}


__ where $k' = \lceil n/b \rceil$. __




---
###<font color =#4C5455><code> Theoretical Background: __consistency of the NBB procedure for__ </code> $\beta_1$ </font> 

<code> Using results from Lahiri (2003), Bradley (2005), Hamilton (1994), Skorokhod (1965), and Billingsley (1999), we can verify that </code>

$$\mathbb{P}^* \left( \sqrt{n}(\hat{\beta}_1^* - \hat{\beta}_1) \leq x \right) 
    \stackrel{p}{\to} \mathbb{P} \left( \sqrt{n}(\hat{\beta}_1 - \beta_1) \leq x \right) \quad \text{as } n \to \infty$$

... after demonstrating that the variance of the bootstrap variance estimator correctly converges to the true variance $V$ and the same asymptotic normality as the original OLS estimator $\sqrt{n} (\hat{\beta}_1 - \beta_1) \stackrel{d}{\to} \mathcal{N}(0, V)$:

<br>

$$\text{Var}^*(\sqrt{n}(\hat{\beta}_1^* - \hat{\beta}_1)) \stackrel{p}{\to} V$$


<br>

$$\sqrt{n} (\hat{\beta}_1^* - \hat{\beta}_1) \stackrel{d^*}{\to} \mathcal{N}(0, V)$$



---
class: inverse, center, middle
<!-- title-slide-section-grey,  -->

## Results for the NBB Method


---
###<font color =#4C5455><code> Empirical Results: __data & methodology__ </code></font>

<br>
<br>

--
> Alpha Vantage and MT5 data, 30-minute timeframe; <br><br>

--
> 2 years of data: __`02-01-2023 - 01-31-2025`__;<br><br>

--
> Backtest consisted of sliding window with 3 periods: forming `[t-1]` | decision `[t]` | trading `[t+1]`;<br><br>

--
> Parameters: sizes $w \in \{65, 130\}$, quantile parameter $\alpha \in \{0.05, 0.15\}$, and block size $b \in \{13, 26, 39, 65\}$ <br><br>

--
> The algorithm was implemented in R


---
###<font color =#4C5455><code> Empirical Results: __backtesting__ </code></font>

<iframe src="animated_z_score_with_spike_and_stabilization6.html" width="750" height="520" style="border:none;"></iframe>


---
###<font color =#4C5455><code> Empirical Results: __U.S.__ </code></font>

<br>
<br>
<br>

.center[

[__LINK__](./results/US3.html)

]



---
###<font color =#4C5455><code> Empirical Results: __Brazil__ </code></font>


<br>
<br>
<br>

.center[

[__LINK__](./results/Brazil3.html)

]


---
class: inverse, center, middle
<!-- title-slide-section-grey,  -->

## Conclusion


---
###<font color =#4C5455><code> Conclusion: __general remarks & future research__ </code></font>

.pull-left[

> Both Bayesian and NBB algorithms significantly outperformed the standard cointegration strategy; 

<br>

> Superior risk metrics: volatility, maximum drawdown, Sharpe, Sortino; 

<br>

> NBB vs. Bayesian: <br>
  > + fewer parameter sensitivities; <br><br>
  > + higher returns; 
    

]

.pull-right[

> Bayesian: 
<br>
  > + Offers a more refined tool for pair selection $\to$ applicability beyond pairs trading. 
    
<br>

> Both methods innaugurate a family of distribution-based strategies in pairs trading 

<br>

> __Future research:__ <br> 
  > + broader range of markets and asset classes; <br>
  > + different parameter configurations; <br>
  > + incorporating transaction costs.

]

---
class: inverse, center, middle
<!-- title-slide-section-grey,  -->

## Equipment Life-time modeling in the absence of failure data


---
###<font color =#4C5455><code> Introduction: __reliability theory__ </code></font>

<br>
<br>

--
Connerstone for performance and safety assurance of complex technological systems; 

<br>
<br>


--
Has its origins in the mid-XX century driven by military needs; 

<br>
<br>

--
Expanded into decision-based framework that prioritize __risk__ $\to$ probabilistic approaches:

<br>

\begin{equation}
    \text{Risk} = \sum_{i} P(E_i) \times C(E_i)
\end{equation}




---
###<font color =#4C5455><code> Proposed method: __motivation__ </code></font>

<br>
<br>

--
> All probabilistic approaches (survival models) pressupose the availability of at least some historical failure data;

<br>

--
> Critical gap in reliability engineering: innability to generate meaningful predictions in contexts of __complete data absence__;<br>

  > + barrier to risk-based management in environments where it would be most valuable: __newly commissioned facilities__; __specialized equipment with limited deployment__ $\to$ case study: National Bio and Agro-Defense Facility (NBAF)
  
  > + alternatives: accelerated life testing; data pooling; surrogate data; physics-based models $\to$ all insufficient.
  


---
###<font color =#4C5455><code> Proposed method: __motivation__ </code></font>

<br>
<br>
<br>

> Bayesian framework initially offers a natural solution to the data-scarcity problem; 
<br>
<br>
  > + Encodes expert judgement, theoretical understanding, and manufacturer specifications into the priors; 
  
  <br>
  
  > + __But__ we still need failure data into the likelihood; <br><br>

--
> __Previous Idea__:<br>
  > + Johnson _et. al._ (2005): hierarchical Bayesian model to estimate early reliability using borrowed data from comparable systems $\to$ __problematic__ $\to$ failure mechanisms are highly context-dependent.



---
###<font color =#4C5455><code> Proposed method: __unorthodox hierarchical Bayesian method__ </code></font>

> __Observation:__ Bayesian models are capable of automatically balancing prior beliefs and evidence from data.</br></br>

--
> __Proposal:__ Create a hierarchical Bayesian model that heavily relies on expert and other operational characteristics before failure data is collected. </br></br>

<!-- ??? As new data enters the system, the model will balance itself automatically without the need of recalibration.</br></br> -->


--

> __How:__ 

  > + `First`: simulate the likelihood function using equipment operational information and synthetizing theoretical considerations, physical constraints, and domain knowledge about failure mechanisms. 
  
  > + `Second`: conduct meticulous prior elicitation based on readily available operational variables 
  

  <!-- such as manufacturer-specified expected lifetime, equipment runtime, maintenance quality, and system criticality. -->
  
  
--
<code> __Hypotheses:__  </code>
  
  > __(i)__ establishes a formal mechanism for transforming qualitative information into quantifiable probabilistic statements;
  
  > __(ii)__ enables model to learn from operational data that preceds failure 
  
  <!-- ??? creates a strucuture for continuously refining predictions based on evolving operational information, even before actual failure occurs  -->
  

---
class: inverse, center, middle
<!-- title-slide-section-grey,  -->

## Implementation

---
###<font color =#4C5455><code> Model: __Bayesian hierarchical model__ </code></font>

<br>
<br>
<br>

For each equipment $i$, we have:

<br>
<br>

\begin{equation}
    t_i \sim \mathit{Wei}(\hat{k}_i, \hat{\lambda}_i) \\
    \text{ }\\
    k_i \sim \mathcal{LN}(\mu_i, \sigma_i^2) \\
    \text{ }\\
    \lambda_i \sim \Gamma(\alpha_{i}, \beta_{i})
\end{equation}


---
###<font color =#4C5455><code> Likelihood: __simulated Weibull__ </code></font>

\begin{equation}
h(t) = \frac{\hat{k}_i}{\hat{\lambda}_i} \left(\frac{t}{\hat{\lambda}_i}\right)^{\hat{k}_i -1}, \quad t > 0, \hat{k}_i > 0, \hat{\lambda}_i > 0
\end{equation}

> We rely on prior knowledge and equipment operational characteristics.<br><br>

> __ $\hat{k}_i$ __ (shape parameter):</br>

  > + recently installed equipment: $\hat{k}_i < 1$ $\to$ early stage failures</br>
  
  > + otherwise: $\hat{k}_i > 1$ $\to$ aging and wear-out failures</br>
  
> __ $\hat{\lambda}_i$ __ (scale parameter or _characteristic life_):</br> 
  
  > + average or median median of the lifetimes provided by the manufacturer for a specific group of similar equipment

---
###<font color =#4C5455><code> Priors: __Gamma__ </code></font>

> Mapped variable: __expected life__ $\xi_i$ (in years)


> Mapping functions:

.pull-left[

\begin{equation}
    \alpha_i = \frac{1}{\Delta_i^2} \\
    \beta_i = \frac{\alpha_i}{\xi_i}
\end{equation}

> $\Delta_i$ measures the dispersion (uncertainty) between manufacturer specifications and engineering assessments for the expected life

]

.pull-right[

\begin{equation}
    \Delta_i = \max\left(\frac{|\xi_i - \hat{\mu}_{\text{eng},i}|}{\xi_i}, c\right)
\end{equation}

\begin{equation}
    \hat{\mu}_{\text{eng},i} = \frac{a_i + b_i}{2}
\end{equation}

> $c > 0$ avoids $\Delta_i = 0$;

> $\hat{\mu}_{\text{eng},i}$ is the midpoint of the estimated minimum and maximum life values of the engineer. 

> $a_i$ minimum estimated life and $b_i$ maximum estimated life for the equipment. 

]



---
###<font color =#4C5455><code> Gamma prior: __sensitivity to input variables__ </code></font>


```{r, fig.height=7, fig.width=10, fig.align='center', fig.retina=12, results='hold'}
# Mapping Function for Gamma based on Expectation Difference

# Model parameters
manufacturer_life <- 10    # Expected life specified by manufacturer
min_threshold <- 0.05      # Minimum threshold for dispersion

# Function that maps expectation difference to Gamma parameters
# ensuring that the mean of the Gamma equals the manufacturer's expected life
map_gamma_params <- function(manufacturer_life, engineer_min, engineer_max, p = 1.5) {
  # Calculate midpoint of engineer's estimate
  engineer_mean <- (engineer_min + engineer_max) / 2
 
  # Calculate relative difference between expectations
  expectation_diff <- abs(manufacturer_life - engineer_mean) / manufacturer_life
 
  # Ensure minimum dispersion for numerical stability
  dispersion <- max(expectation_diff, min_threshold)
 
  # Calculate alpha to control shape
  alpha <- 1 / dispersion^2
 
  # Calculate beta to ensure mean = manufacturer_life
  beta <- alpha / manufacturer_life
 
  return(list(alpha = alpha, beta = beta, dispersion = dispersion))
}

# Create a single figure with 4 plots in quadrants
create_quadrant_plots <- function() {
  # Configure layout
  par(mfrow = c(2, 2))
 
  # FIRST ROW: Different Expected Lives
  # Define different manufacturer expected lives
  manufacturer_lives <- c(5, 10, 15, 20, 25)
 
  # We'll use a consistent engineer assessment relationship:
  # Engineer consistently estimates 20% lower and 10% higher than manufacturer
  engineer_ests <- list()
  for (i in 1:length(manufacturer_lives)) {
    lower <- manufacturer_lives[i] * 0.8  # 20% lower
    upper <- manufacturer_lives[i] * 1.1  # 10% higher
    engineer_ests[[i]] <- c(lower, upper)
  }
 
  # Create a color vector for the different lives
  colors_lives <- rainbow(length(manufacturer_lives))
 
  # Store calculated parameters
  params_lives <- list()
  for (i in 1:length(manufacturer_lives)) {
    params_lives[[i]] <- map_gamma_params(
      manufacturer_lives[i],
      engineer_ests[[i]][1],
      engineer_ests[[i]][2]
    )
  }
 
  # Calculate statistics for each distribution
  alphas_lives <- sapply(params_lives, function(p) p$alpha)
  betas_lives <- sapply(params_lives, function(p) p$beta)
 
  # Generate samples and determine visualization limits
  max_x_value_lives <- max(manufacturer_lives) * 3
 
  # Plot 1 (Top Left): PDF of Gamma distributions for different lives
  plot(NULL,
       xlim = c(0, max_x_value_lives),
       ylim = c(0, 0.5),
       xlab = "Value",
       ylab = "Probability Density",
       main = "PDF for Different Expected Lives")
 
  # Legend text
  legend_text_lives <- c()
 
  for (i in 1:length(manufacturer_lives)) {
    alpha <- alphas_lives[i]
    beta <- betas_lives[i]
    life <- manufacturer_lives[i]
   
    # Generate values for the x-axis
    x_plot <- seq(0, max_x_value_lives, length.out = 1000)
   
    # Calculate the PDF for this Gamma distribution
    pdf_values <- dgamma(x_plot, shape = alpha, rate = beta)
   
    # Add to the graph
    lines(x_plot, pdf_values, col = colors_lives[i], lwd = 2)
   
    # Mark mean with vertical line
    abline(v = life, col = colors_lives[i], lty = 2)
   
    # Legend text
    legend_text_lives <- c(legend_text_lives, sprintf("Life = %d", life))
  }
 
  # Add legend
  legend("topright", legend = legend_text_lives, col = colors_lives, lwd = 2, cex = 0.7)
  grid()
 
  # Plot 2 (Top Right): CDF of Gamma distributions for different lives
  plot(NULL,
       xlim = c(0, max_x_value_lives),
       ylim = c(0, 1),
       xlab = "Value",
       ylab = "Cumulative Probability",
       main = "CDF for Different Expected Lives")
 
  for (i in 1:length(manufacturer_lives)) {
    alpha <- alphas_lives[i]
    beta <- betas_lives[i]
    life <- manufacturer_lives[i]
   
    # Use the same range of x values
    x_plot <- seq(0, max_x_value_lives, length.out = 1000)
   
    # Calculate the CDF for this Gamma distribution
    cdf_values <- pgamma(x_plot, shape = alpha, rate = beta)
   
    # Add to the graph
    lines(x_plot, cdf_values, col = colors_lives[i], lwd = 2)
   
    # Mark mean with vertical line
    abline(v = life, col = colors_lives[i], lty = 2)
  }
 
  # Add legend
  legend("bottomright", legend = legend_text_lives, col = colors_lives, lwd = 2, cex = 0.7)
  grid()
 
  # SECOND ROW: Different Dispersion Levels
  # Fixed manufacturer life
  fixed_life <- 10
 
  # Different dispersion levels to explore
  dispersion_levels <- c(0.05, 0.1, 0.2, 0.3, 0.5)
 
  # Create a color vector for the different dispersion levels
  colors_disp <- rainbow(length(dispersion_levels))
 
  # Calculate parameters directly from dispersion levels
  alphas_disp <- numeric(length(dispersion_levels))
  betas_disp <- numeric(length(dispersion_levels))
 
  for (i in 1:length(dispersion_levels)) {
    disp <- dispersion_levels[i]
    alphas_disp[i] <- 1 / disp^2
    betas_disp[i] <- alphas_disp[i] / fixed_life
  }
 
  # Calculate visualization limits
  max_x_value_disp <- fixed_life * 3
 
  # Plot 3 (Bottom Left): PDF for different dispersion levels
  plot(NULL,
       xlim = c(0, max_x_value_disp),
       ylim = c(0, 1.5),
       xlab = "Value",
       ylab = "Probability Density",
       main = "PDF for Different Dispersion Levels")
 
  # Legend text
  legend_text_disp <- c()
 
  for (i in 1:length(dispersion_levels)) {
    alpha <- alphas_disp[i]
    beta <- betas_disp[i]
    disp <- dispersion_levels[i]
   
    # Generate values for the x-axis
    x_plot <- seq(0, max_x_value_disp, length.out = 1000)
   
    # Calculate the PDF for this Gamma distribution
    pdf_values <- dgamma(x_plot, shape = alpha, rate = beta)
   
    # Add to the graph
    lines(x_plot, pdf_values, col = colors_disp[i], lwd = 2)
   
    # Legend text
    legend_text_disp <- c(legend_text_disp, sprintf("Disp = %.2f", disp))
  }
 
  # Add vertical line at manufacturer life
  abline(v = fixed_life, lty = 2, col = "black")
  text(fixed_life * 1.1, 1.3, paste("Fixed Life =", fixed_life), adj = 0, cex = 0.8)
 
  # Add legend
  legend("topright", legend = legend_text_disp, col = colors_disp, lwd = 2, cex = 0.7)
  grid()
 
  # Plot 4 (Bottom Right): CDF for different dispersion levels
  plot(NULL,
       xlim = c(0, max_x_value_disp),
       ylim = c(0, 1),
       xlab = "Value",
       ylab = "Cumulative Probability",
       main = "CDF for Different Dispersion Levels")
 
  for (i in 1:length(dispersion_levels)) {
    alpha <- alphas_disp[i]
    beta <- betas_disp[i]
   
    # Use the same range of x values
    x_plot <- seq(0, max_x_value_disp, length.out = 1000)
   
    # Calculate the CDF for this Gamma distribution
    cdf_values <- pgamma(x_plot, shape = alpha, rate = beta)
   
    # Add to the graph
    lines(x_plot, cdf_values, col = colors_disp[i], lwd = 2)
  }
 
  # Add vertical line at manufacturer life
  abline(v = fixed_life, lty = 2, col = "black")
 
  # Add legend
  legend("bottomright", legend = legend_text_disp, col = colors_disp, lwd = 2, cex = 0.7)
  grid()
 
  # Restore default graph configuration
  par(mfrow = c(1, 1))
 
  # Print statistics for different lives
  # cat("\nStatistics for distributions with different expected lives:\n")
  # cat("-------------------------------------------------------------------\n")
  # cat("Manuf. Life | Eng. Range | Alpha | Beta | Mean | Variance | CV\n")
  # cat("-------------------------------------------------------------------\n")
 
  for (i in 1:length(manufacturer_lives)) {
    life <- manufacturer_lives[i]
    eng_range <- sprintf("%.1f-%.1f", engineer_ests[[i]][1], engineer_ests[[i]][2])
    alpha <- alphas_lives[i]
    beta <- betas_lives[i]
    mean_val <- alpha / beta
    variance <- alpha / (beta^2)
    cv <- sqrt(variance) / mean_val
   
    # cat(sprintf("%d | %s | %.2f | %.4f | %.2f | %.2f | %.3f\n",
    #             life, eng_range, alpha, beta, mean_val, variance, cv))
  }
 
  # Print statistics for different dispersion levels
  # cat("\nStatistics for distributions with different dispersion levels:\n")
  # cat("-------------------------------------------------------------------\n")
  # cat("Dispersion | Alpha | Beta | Mean | Variance | CV | Mode\n")
  # cat("-------------------------------------------------------------------\n")
 
  for (i in 1:length(dispersion_levels)) {
    disp <- dispersion_levels[i]
    alpha <- alphas_disp[i]
    beta <- betas_disp[i]
    mean_val <- alpha / beta
    variance <- alpha / (beta^2)
    cv <- sqrt(variance) / mean_val
    mode <- if(alpha > 1) (alpha - 1) / beta else "NA"
   
    # cat(sprintf("%.2f | %.2f | %.4f | %.2f | %.2f | %.3f | %s\n",
    #             disp, alpha, beta, mean_val, variance, cv, mode))
  }
}

# Run the function to create the quadrant plots
create_quadrant_plots()
```


---
###<font color =#4C5455><code> Priors: __Log-normal__ </code></font>

<br>
<br>

> Mapped variables: __age, runtime, maintenance quality__, and __criticality__ as a proportion $x$ of the expected life, i.e. [0,1].


> Mapping functions:

<br>

\begin{equation}
    \sigma_{k_i}^2 = x^2 \\
    \label{log-mu}
    \mu_{k_i} = \log(\hat{k}_i) \times C - \frac{\sigma_{k_i}^2}{2}
\end{equation}

> $\hat{k}_i$ represents the baseline failure rate for the analyzed equipment;

> $x$ is the proportion of the expected life for the given input variable;

> $C$ is a scaling factor to make the survival curve more or less responsive.

---
###<font color =#4C5455><code> Log-normal prior: __sensitivity to input variables__ </code></font>

```{r, fig.height=7, fig.width=10, fig.align='center', fig.retina=12, results='hold'}
# Mapping Function for Log-normal with Constant Mean (Adapted)

# Model parameters
k <- 2.5    # desired constant value for the log-normal mean
C <- 1      # C value as requested

# Function that maps the proportion of expected life (x) to log-normal parameters
# maintaining the mean constant at k
map_lognormal_params <- function(x, k) {
  # The variance parameter increases with the proportion of expected life
  # We can use a function that grows with x
  sigma2 <- x^2  # We keep the same quadratic relationship of the original model
 
  # We calculate mu to ensure that the log-normal mean is k
  # For a log-normal: E[X] = exp(mu + sigma2/2)
  # To get E[X] = k, we need:
  # exp(mu + sigma2/2) = k
  # mu + sigma2/2 = log(k)
  # mu = log(k) - sigma2/2
  mu <- log(k) - sigma2/2
 
  return(list(mu = mu, sigma2 = sigma2))
}

# Simulate log-normal distributions for different proportions of expected life
simulate_distributions <- function() {
  # Define x values (proportion of expected life)
  x_values <- seq(0.1, 0.9, by = 0.2)  # 0.1, 0.3, 0.5, 0.7, 0.9
 
  # Create a color vector for the different proportions
  colors <- rainbow(length(x_values))
 
  # Store calculated parameters
  params <- list()
  for (i in 1:length(x_values)) {
    params[[i]] <- map_lognormal_params(x_values[i], k)
  }
 
  # Configure graph layout
  par(mfrow = c(2, 2))
 
  # Plot 1: Visualization of mu behavior
  mu_values <- sapply(params, function(p) p$mu)
  plot(x_values,
       mu_values,
       type = "b",
       col = "blue",
       xlab = "Proportion of Expected Life (x)",
       ylab = expression("Value of " ~ mu),
       main = expression("Behavior of " ~ mu ~" vs. Proportion"))
  grid()
 
  # Plot 2: Visualization of sigma² behavior
  sigma2_values <- sapply(params, function(p) p$sigma2)
  plot(x_values,
       sigma2_values,
       type = "b",
       col = "red",
       xlab = "Proportion of Expected Life (x)",
       ylab = expression("Value of " ~ sigma^2),
       main = expression("Behavior of " ~ sigma^2 ~" vs. Proportion"))
  grid()
 
  # Calculate statistics and generate samples for each distribution
  set.seed(123)  # For reproducibility
  samples <- list()
  max_density <- 0
  modes <- numeric(length(x_values))
  means <- numeric(length(x_values))
  medians <- numeric(length(x_values))
  pdf_max_values <- numeric(length(x_values))
 
  # Determine limits for visualization
  max_x_value <- 0
 
  for (i in 1:length(x_values)) {
    mu <- params[[i]]$mu
    sigma2 <- params[[i]]$sigma2
   
    # Generate sample for each distribution (following code 2)
    samples[[i]] <- rlnorm(10000, meanlog = mu, sdlog = sqrt(sigma2))
   
    # Calculate important statistics
    modes[i] <- exp(mu - sigma2)  # Mode of log-normal
    means[i] <- exp(mu + sigma2/2)  # Mean of log-normal
    medians[i] <- exp(mu)  # Median of log-normal
   
    # Determine a reasonable upper limit for the x-axis
    q99 <- quantile(samples[[i]], 0.99)
    if (q99 > max_x_value) max_x_value <- q99
   
    # Calculate the maximum density value to adjust the y-axis
    mode_density <- dlnorm(modes[i], meanlog = mu, sdlog = sqrt(sigma2))
    pdf_max_values[i] <- mode_density
    if (mode_density > max_density) max_density <- mode_density
  }
 
  # Adjust the upper limit to not be excessively large
  max_x_value <- min(max_x_value, 20)
 
  # Plot 3: PDF of Log-normal distributions
  plot(NULL,
       xlim = c(0, max_x_value),
       ylim = c(0, max_density * 1.1),
       xlab = "Value",
       ylab = "Probability Density",
       main = "Probability Density Function (PDF)")
 
  legend_text <- c()
 
  for (i in 1:length(x_values)) {
    mu <- params[[i]]$mu
    sigma2 <- params[[i]]$sigma2
   
    # Generate values for the x-axis based on the sample (following code 2)
    x_plot <- seq(0, max_x_value, length.out = 1000)
   
    # Calculate the PDF for this Log-normal distribution
    pdf_values <- dlnorm(x_plot, meanlog = mu, sdlog = sqrt(sigma2))
   
    # Add to the graph
    lines(x_plot, pdf_values, col = colors[i], lwd = 2)
   
    # Mark important points
    points(modes[i], pdf_max_values[i], col = colors[i], pch = 16, cex = 1.2)  # Mode (circle)
    points(means[i], dlnorm(means[i], mu, sqrt(sigma2)), col = colors[i], pch = 17, cex = 1.2)  # Mean (triangle)
   
    # Text for the legend
    legend_text <- c(legend_text, sprintf("x = %.1f, \u03BC = %.2f, \u03C3\u00B2 = %.2f",
                                          x_values[i], mu, sigma2))
  }
 
  # Add vertical line at constant mean k
  abline(v = k, lty = 2, col = "black")
  text(k * 1.1, max_density * 0.8, paste("k =", k), adj = 0)
 
  # Add legend
  legend("topright", legend = legend_text, col = colors, lwd = 2, cex = 0.7)
  grid()
 
  # Plot 4: CDF of Log-normal distributions
  plot(NULL,
       xlim = c(0, max_x_value),
       ylim = c(0, 1),
       xlab = "Value",
       ylab = "Cumulative Probability",
       main = "Cumulative Distribution Function (CDF)")
 
  for (i in 1:length(x_values)) {
    mu <- params[[i]]$mu
    sigma2 <- params[[i]]$sigma2
   
    # Use the same range of x values from the PDF graph
    x_plot <- seq(0, max_x_value, length.out = 1000)
   
    # Calculate the CDF for this Log-normal distribution
    cdf_values <- plnorm(x_plot, meanlog = mu, sdlog = sqrt(sigma2))
   
    # Add to the graph
    lines(x_plot, cdf_values, col = colors[i], lwd = 2)
   
    # Mark median
    points(medians[i], 0.5, col = colors[i], pch = 16, cex = 1.2)
  }
 
  # Add vertical line at constant mean k
  abline(v = k, lty = 2, col = "black")
 
  # Add legend
  legend("bottomright", legend = legend_text, col = colors, lwd = 2, cex = 0.7)
  grid()
 
  # Restore default graph configuration
  par(mfrow = c(1, 1))
 
  # Print calculated statistics
  # cat("\nStatistics for distributions with different proportions of expected life:\n")
  # cat("-------------------------------------------------------------------\n")
  # cat("Proportion (x) | Mean | Median | Mode | Empirical Mean\n")
  # cat("-------------------------------------------------------------------\n")
 
  for (i in 1:length(x_values)) {
    # Calculate empirical mean from samples
    empirical_mean <- mean(samples[[i]])
   
    # cat(sprintf("%.1f | %.2f | %.2f | %.2f | %.2f\n",
    #             x_values[i], means[i], medians[i], modes[i], empirical_mean))
  }
}

# Run the simulation
simulate_distributions()

```


---
class: inverse, center, middle
<!-- title-slide-section-grey,  -->

## Results


---
###<font color =#4C5455><code> Results: __NBAF App__ </code></font>

<br>
<br>
<br>

.center[

[__LINK__](https://allanvc.shinyapps.io/nbaf-app)

]


---
class: inverse, center, middle
<!-- title-slide-section-grey,  -->

## Conclusions

---
###<font color =#4C5455><code> Conclusions </code></font>

> The proposed framework effectively circumvents the problem of scarce historical data; <br><br>

> It integrates seamlessly with reliability theory; <br><br>

> Restrictions:<br>
  > + highly dependent on prior beliefs;<br>
  > + disproportional weight to expected life parmeter (prior and likelihood);<br><br>
  
> Future research: <br>
  > Improve prior elicitation and validation;<br>
  > Room to incorporate time-dependent covariates and other environmental factors;<br>
  > Expand the hierarchical levels of the model;
  > Empirical validation



---
class: inverse, center, middle
<!-- title-slide-section-grey,  -->

## Thank you!